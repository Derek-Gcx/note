# 参数学习
+ 在贝叶斯网络中，进行推理的前提是已知网络的结构和参数。
+ 但是模型和参数未知的前提下，我们需要从给定的样例中学习参数和模型结构

## 极大似然参数学习：离散模型
### 二项分布
+ 二项分布只有一个参数$\theta=P(C=1)$
+ 假设D是一个数据集，是对该二项分布的一个**抽样**
+ 则似然为$\hat{\theta}=\argmax_\theta P(D|\theta)=\argmax_\theta C_n^m\theta^m(1-\theta)^n=\argmax_\theta L(\theta)$, $L(\theta)$即为**似然函数**
+ 实际上我们经常最大化**对数似然函数**$\ln L(\theta)\propto m\ln \theta+(n-m)\ln(1-\theta)$，求梯度即可
+ (很多情况下)如果有对待学习参量的约束，那么可看作带约束优化问题，拉格朗日乘子法即可

## 极大似然参数学习：连续模型
### 高斯分布
$\mathcal{N}(x|\mu, \sigma^2)=\frac 1{\sqrt{2\pi}\sigma}\exp(\frac{(x-\mu)^2}{2\sigma^2})$
+ 则对数似然为  
  ![](img/2020-03-12-21-08-51.png)
+ 极大似然估计为
  $\hat{\mu}=\frac 1n\sum_{j=1}^nx_j$
  $\hat{\sigma}^2=\frac 1n\sum_{j=1}^n(x_j-\hat{\mu})^2$

### 线性高斯模型
$P(x|y) = \mathcal{N}(x|my+b, \sigma^2)$
+ 此时参数学习即为从数据集$(y_{1:n}, x_{1:n})$中学习参数$m, b, \sigma$

## 贝叶斯参数学习
+ 贝叶斯学习
  + 给定数据，计算每个假说的概率，并基于这些概率做决策
  + 用所有假说预测，而**不是使用单个表现最好的假说**
  + 把学习规约于概率推理
+ 极大后验学习（Maximum A Posteriori）
  + 基于**单个最可能假说（极大后验假说）**做预测
  + 当假说先验均匀分布时，规约为一个极大似然假说
  + 更容易，无需解决一个大规模求和/积分问题

### 二项分布



## 非参数化模型的密度估算